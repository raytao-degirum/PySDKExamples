{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c15cb24",
   "metadata": {},
   "source": [
    "![Degirum banner](https://raw.githubusercontent.com/DeGirum/PySDKExamples/main/images/degirum_banner.png)\n",
    "## AI Inference on many video files\n",
    "This notebook is an example of how to use DeGirum PySDK to do AI inference of multiple video streams from video files multiplexing frames. This example demonstrates lowest possible and stable AI inference latency while maintaining decent throughput. This is achieved by using synchronous prediction mode and video decoding offloaded into separate thread.\n",
    "\n",
    "This script works with the following inference options:\n",
    "\n",
    "1. Run inference on DeGirum Cloud Platform;\n",
    "2. Run inference on DeGirum AI Server deployed on a localhost or on some computer in your LAN or VPN;\n",
    "3. Run inference on DeGirum ORCA accelerator directly installed on your computer.\n",
    "\n",
    "To try different options, you need to specify the appropriate `hw_location` option.\n",
    "\n",
    "When running this notebook locally, you need to specify your cloud API access token in the [env.ini](../../env.ini) file, located in the same directory as this notebook.\n",
    "\n",
    "When running this notebook in Google Colab, the cloud API access token should be stored in a user secret named `DEGIRUM_CLOUD_TOKEN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b5bfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure degirum-tools package is installed\n",
    "!pip show degirum-tools || pip install degirum-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01549d7c-2445-4007-8a89-ac0f3a864530",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Specify where you want to run inferences, model_zoo_url, model_name, video file names, and other options here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c959bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hw_location: where you want to run inference\n",
    "#     \"@cloud\" to use DeGirum cloud\n",
    "#     \"@local\" to run on local machine\n",
    "#     IP address for AI server inference\n",
    "# model_zoo_url: url/path for model zoo\n",
    "#     cloud_zoo_url: valid for @cloud, @local, and ai server inference options\n",
    "#     '': ai server serving models from local folder\n",
    "#     path to json file: single model zoo in case of @local inference\n",
    "# model_name: name of the model for running AI inference\n",
    "# input_filenames: paths to video files for inference\n",
    "# offload_preprocessing: True to do image preprocessing outside of inference call\n",
    "# do_image_compression: True to do JPEG compression before sending image for inference\n",
    "hw_location = \"@cloud\"\n",
    "model_zoo_url = \"degirum/public\"\n",
    "model_name = \"mobilenet_v2_ssd_coco--300x300_quant_n2x_orca1_1\"\n",
    "input_filenames = [\n",
    "    \"https://raw.githubusercontent.com/DeGirum/PySDKExamples/main/images/Traffic.mp4\",\n",
    "    \"https://raw.githubusercontent.com/DeGirum/PySDKExamples/main/images/Traffic.mp4\",\n",
    "    \"https://raw.githubusercontent.com/DeGirum/PySDKExamples/main/images/Traffic.mp4\",\n",
    "    \"https://raw.githubusercontent.com/DeGirum/PySDKExamples/main/images/Traffic.mp4\",\n",
    "]\n",
    "offload_preprocessing = True  # do image preprocessing outside of inference call\n",
    "do_image_compression = True  # do JPEG compression before sending image for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c6d38b-d22a-45ab-910d-cf3d4f2dd9a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The rest of the cells below should run without any modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5603895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg, degirum_tools\n",
    "import cv2, numpy, time, threading, queue\n",
    "from contextlib import ExitStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b7d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stream multiplexing source:\n",
    "# it reads frames from given video files in round robin fashion\n",
    "# and puts them into given queue.\n",
    "# If offload_preprocessing is enabled, it also performs image resizing\n",
    "def mux_source(streams, frame_queue, model):\n",
    "    phase = 0  # stream multiplexing phase counter\n",
    "\n",
    "    while True:\n",
    "        ret, frame = streams[phase].read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if offload_preprocessing:\n",
    "            # do image resizing\n",
    "            frame = model._preprocessor.forward(frame)[0]\n",
    "\n",
    "        frame_queue.put((frame, phase))\n",
    "\n",
    "        phase = (phase + 1) % len(streams)  # advance mux phase\n",
    "\n",
    "    frame_queue.put(None)  # send poison pill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031948f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with dg.load_model(\n",
    "    model_name=model_name,\n",
    "    inference_host_address=hw_location,\n",
    "    zoo_url=model_zoo_url,\n",
    "    token=degirum_tools.get_token(),\n",
    "    ) as model, ExitStack() as stack:\n",
    "    # create model object in `with` block to avoid server disconnections on each frame inference\n",
    "\n",
    "    model.input_image_format = \"JPEG\" if do_image_compression else \"RAW\"\n",
    "    model.measure_time = True\n",
    "\n",
    "    # open video streams\n",
    "    streams = [\n",
    "        stack.enter_context(degirum_tools.open_video_stream(fn))\n",
    "        for fn in input_filenames\n",
    "    ]\n",
    "\n",
    "    frame_queue = queue.Queue(maxsize=10)  # queue to enqueue frames\n",
    "    start_times = []  # list of frame starting times\n",
    "    end_times = []  # list of frame result receiving times\n",
    "\n",
    "    # start frame retrieving thread\n",
    "    mux_tread = threading.Thread(target=mux_source, args=(streams, frame_queue, model))\n",
    "    mux_tread.start()\n",
    "\n",
    "    # initialize progress indicator\n",
    "    steps = min([stream.get(cv2.CAP_PROP_FRAME_COUNT) for stream in streams])\n",
    "    progress = degirum_tools.Progress(steps * len(streams))\n",
    "\n",
    "    # inference loop\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        # get frame from queue\n",
    "        frame = frame_queue.get()\n",
    "        if frame is None:\n",
    "            break  # got poison pill: end loop\n",
    "\n",
    "        # do inference and record times\n",
    "        start_times.append(time.time())\n",
    "        res = model(frame[0])\n",
    "        end_times.append(time.time())\n",
    "\n",
    "        progress.step()\n",
    "\n",
    "    mux_tread.join()\n",
    "\n",
    "    # print time statistics\n",
    "    for s in sorted(model.time_stats().items()):\n",
    "        print(s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888a7924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process latency times\n",
    "end_times = numpy.array(end_times)\n",
    "start_times = numpy.array(start_times)\n",
    "latency_times_ms = (end_times - start_times) * 1000\n",
    "\n",
    "print(\"\\nLatency Histogram\")\n",
    "latency_hist = numpy.histogram(latency_times_ms)\n",
    "for hval, bin in zip(latency_hist[0], latency_hist[1]):\n",
    "    print(f\"{bin:4.0f} ms:     {hval:4}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (supervision)",
   "language": "python",
   "name": "supervision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
