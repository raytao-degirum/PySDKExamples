{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Degirum banner](https://raw.githubusercontent.com/DeGirum/PySDKExamples/main/images/degirum_banner.png)\n",
    "## Multi-Source and Multi-Model AI Inference\n",
    "This notebook is an example of how to perform AI inferences of multiple models processing multiple video streams.\n",
    "Each video stream is fed to every model. Each model processes frames from every video stream in a multiplexing manner.\n",
    "\n",
    "This script works with the following inference options:\n",
    "\n",
    "1. Run inference on DeGirum Cloud Platform;\n",
    "2. Run inference on DeGirum AI Server deployed on a localhost or on some computer in your LAN or VPN;\n",
    "3. Run inference on DeGirum ORCA accelerator directly installed on your computer.\n",
    "\n",
    "To try different options, you need to specify the appropriate `hw_location` option. \n",
    "\n",
    "When running this notebook locally, you need to specify your cloud API access token in the [env.ini](../../env.ini) file, located in the same directory as this notebook.\n",
    "\n",
    "When running this notebook in Google Colab, the cloud API access token should be stored in a user secret named `DEGIRUM_CLOUD_TOKEN`.\n",
    "\n",
    "The script can use a web camera(s) or local camera(s) connected to the machine running this code or it can use video file(s).\n",
    "The camera index or URL or video file path should be specified in the code below by assigning `video_sources`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: degirum_tools\n",
      "Version: 0.10.1\n",
      "Summary: Tools for PySDK\n",
      "Home-page: \n",
      "Author: DeGirum\n",
      "Author-email: \n",
      "License: \n",
      "Location: c:\\users\\shashichilappagari\\anaconda3\\envs\\supervision\\lib\\site-packages\n",
      "Requires: degirum, ipython, numpy, opencv-python, pafy, pillow, psutil, pycocotools, python-dotenv, pyyaml, requests, scipy, youtube-dl\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# make sure degirum-tools package is installed\n",
    "!pip show degirum-tools || pip install degirum-tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify video sources and AI model names here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hw_location: where you want to run inference\n",
    "#     \"@cloud\" to use DeGirum cloud\n",
    "#     \"@local\" to run on local machine\n",
    "#     IP address for AI server inference\n",
    "# video_sources: list of video sources\n",
    "#     camera index for local camera\n",
    "#     URL of RTSP stream\n",
    "#     URL of YouTube Video\n",
    "#     path to video file (mp4 etc)\n",
    "# model_zoo_url: url/path for model zoo\n",
    "#     cloud_zoo_url: valid for @cloud, @local, and ai server inference options\n",
    "#     '': ai server serving models from local folder\n",
    "#     path to json file: single model zoo in case of @local inference\n",
    "# model_names: list of AI models to use for inferences (NOTE: they should have the same input size)\n",
    "# allow_frame_drop:\n",
    "#     when True, we drop video frames in case when AI performance is not enough to work in real time\n",
    "#     when False, we buffer video frames to keep up with AI performance\n",
    "hw_location = \"@cloud\"\n",
    "video_sources = [\n",
    "    \"https://raw.githubusercontent.com/DeGirum/PySDKExamples/main/images/Traffic.mp4\",\n",
    "    \"https://raw.githubusercontent.com/DeGirum/PySDKExamples/main/images/TrafficHD.mp4\",\n",
    "]\n",
    "model_zoo_url = \"degirum/public\"\n",
    "model_names = [\n",
    "    \"yolo_v5s_hand_det--512x512_quant_n2x_orca1_1\",\n",
    "    \"yolo_v5s_face_det--512x512_quant_n2x_orca1_1\",\n",
    "    \"yolo_v5n_car_det--512x512_quant_n2x_orca1_1\",\n",
    "    \"yolo_v5s_person_det--512x512_quant_n2x_orca1_1\",\n",
    "]\n",
    "allow_frame_drop = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify where do you want to run your inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import degirum as dg, degirum_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened video stream 'https://raw.githubusercontent.com/DeGirum/PySDKExamples/main/images/Traffic.mp4'Successfully opened video stream 'https://raw.githubusercontent.com/DeGirum/PySDKExamples/main/images/TrafficHD.mp4'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "packet queue is empty, aborting\n",
      "packet queue is empty, aborting\n",
      "packet queue is empty, aborting\n",
      "packet queue is empty, aborting\n"
     ]
    }
   ],
   "source": [
    "from degirum_tools import streams as dgstreams\n",
    "\n",
    "c = dgstreams.Composition()\n",
    "\n",
    "batch_size = len(\n",
    "    video_sources\n",
    ")  # set AI server batch size equal to the # of video sources for lowest latency\n",
    "\n",
    "# create PySDK AI model objects\n",
    "models = []\n",
    "for mi, model_name in enumerate(model_names):\n",
    "    model = dg.load_model(\n",
    "        model_name=model_name,\n",
    "        inference_host_address=hw_location,\n",
    "        zoo_url=model_zoo_url,\n",
    "        token=degirum_tools.get_token(),\n",
    "    )\n",
    "    model.measure_time = True\n",
    "    model.eager_batch_size = batch_size\n",
    "    model.frame_queue_depth = batch_size\n",
    "    models.append(model)\n",
    "\n",
    "# check that all models have the same input configuration\n",
    "models_have_same_input = True\n",
    "for model in models[1:]:\n",
    "    if (\n",
    "        type(model._preprocessor) != type(models[0]._preprocessor)\n",
    "        or model.model_info.InputH != models[0].model_info.InputH\n",
    "        or model.model_info.InputW != models[0].model_info.InputW\n",
    "    ):\n",
    "        models_have_same_input = False\n",
    "\n",
    "resizers = []\n",
    "\n",
    "# create video sources and image resizers\n",
    "# (we use separate resizers to do resize only once per source when possible, to improve performance),\n",
    "# connect each resizer to corresponding video source\n",
    "for src in video_sources:\n",
    "    source = c.add(dgstreams.VideoSourceGizmo(src))\n",
    "    if models_have_same_input:\n",
    "        resizer = c.add(\n",
    "            dgstreams.AiPreprocessGizmo(\n",
    "                models[0], stream_depth=2, allow_drop=allow_frame_drop\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        resizer = c.add(dgstreams.FanoutGizmo(allow_drop=allow_frame_drop))\n",
    "\n",
    "    resizer.connect_to(source)  # connect resizer to video source\n",
    "    resizers.append(resizer)\n",
    "\n",
    "# create result combiner\n",
    "combiner = c.add(dgstreams.AiResultCombiningGizmo(len(models)))\n",
    "\n",
    "# create multi-input detector gizmos,\n",
    "# connect each detector gizmo to every resizer gizmo,\n",
    "# connect result combiner gizmo to each detector gizmo\n",
    "for mi, model in enumerate(models):\n",
    "    # create AI gizmo (aka detector) from the model\n",
    "    detector = c.add(\n",
    "        dgstreams.AiSimpleGizmo(model, stream_depth=2, inp_cnt=len(video_sources))\n",
    "    )\n",
    "\n",
    "    # connect detector gizmo to each resizer gizmo\n",
    "    for fi, resizer in enumerate(resizers):\n",
    "        detector.connect_to(resizer, fi)\n",
    "\n",
    "    # connect result combiner gizmo to detector gizmo\n",
    "    combiner.connect_to(detector, mi)\n",
    "\n",
    "# create multi-window video multiplexing display gizmo\n",
    "# and connect it to combiner gizmo\n",
    "win_captions = [f\"Stream #{i}: {str(src)}\" for i, src in enumerate(video_sources)]\n",
    "display = c.add(\n",
    "    dgstreams.VideoDisplayGizmo(\n",
    "        win_captions, show_ai_overlay=True, show_fps=True, multiplex=True\n",
    "    )\n",
    ")\n",
    "display.connect_to(combiner)\n",
    "\n",
    "# start composition\n",
    "c.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (supervision)",
   "language": "python",
   "name": "supervision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
