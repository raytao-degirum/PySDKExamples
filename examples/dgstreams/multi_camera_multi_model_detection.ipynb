{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Degirum banner](https://raw.githubusercontent.com/DeGirum/PySDKExamples/main/images/degirum_banner.png)\n",
    "## Multi-Source and Multi-Model AI Inference\n",
    "This notebook is an example of how to perform AI inferences of multiple models processing multiple video streams.\n",
    "Each video stream is fed to every model. Each model processes frames from every video stream in a multiplexing manner.\n",
    "\n",
    "This script works with the following inference options:\n",
    "\n",
    "1. Run inference on DeGirum Cloud Platform;\n",
    "2. Run inference on DeGirum AI Server deployed on a localhost or on some computer in your LAN or VPN;\n",
    "3. Run inference on DeGirum ORCA accelerator directly installed on your computer.\n",
    "\n",
    "To try different options, you need to specify the appropriate `hw_location` option. \n",
    "\n",
    "When running this notebook locally, you need to specify your cloud API access token in the [env.ini](../../env.ini) file, located in the same directory as this notebook.\n",
    "\n",
    "When running this notebook in Google Colab, the cloud API access token should be stored in a user secret named `DEGIRUM_CLOUD_TOKEN`.\n",
    "\n",
    "The script can use a web camera(s) or local camera(s) connected to the machine running this code or it can use video file(s).\n",
    "The camera index or URL or video file path should be specified in the code below by assigning `video_sources`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure degirum-tools package is installed\n",
    "!pip show degirum-tools || pip install degirum-tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify video sources and AI model names here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hw_location: where you want to run inference\n",
    "#     \"@cloud\" to use DeGirum cloud\n",
    "#     \"@local\" to run on local machine\n",
    "#     IP address for AI server inference\n",
    "# video_sources: list of video sources\n",
    "#     camera index for local camera\n",
    "#     URL of RTSP stream\n",
    "#     URL of YouTube Video\n",
    "#     path to video file (mp4 etc)\n",
    "# model_zoo_url: url/path for model zoo\n",
    "#     cloud_zoo_url: valid for @cloud, @local, and ai server inference options\n",
    "#     '': ai server serving models from local folder\n",
    "#     path to json file: single model zoo in case of @local inference\n",
    "# model_names: list of AI models to use for inferences (NOTE: they should have the same input size)\n",
    "hw_location = \"@cloud\"\n",
    "video_sources = [\n",
    "    \"https://raw.githubusercontent.com/DeGirum/PySDKExamples/main/images/WalkingPeople.mp4\",\n",
    "    \"https://raw.githubusercontent.com/DeGirum/PySDKExamples/main/images/Traffic.mp4\",\n",
    "]\n",
    "model_zoo_url = \"degirum/public\"\n",
    "model_names = [\n",
    "    \"yolo_v5s_hand_det--512x512_quant_n2x_orca1_1\",\n",
    "    \"yolo_v5s_face_det--512x512_quant_n2x_orca1_1\",\n",
    "    \"yolo_v5n_car_det--512x512_quant_n2x_orca1_1\",\n",
    "    \"yolo_v5s_person_det--512x512_quant_n2x_orca1_1\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The rest of the cells below should run without any modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import degirum as dg, degirum_tools\n",
    "from degirum_tools import streams as dgstreams\n",
    "\n",
    "# create PySDK AI model objects\n",
    "models = [\n",
    "    dg.load_model(\n",
    "        model_name=model_name,\n",
    "        inference_host_address=hw_location,\n",
    "        zoo_url=model_zoo_url,\n",
    "        token=degirum_tools.get_token(),\n",
    "        overlay_line_width=2,\n",
    "    )\n",
    "    for model_name in model_names\n",
    "]\n",
    "\n",
    "# check that all models have the same input configuration\n",
    "assert all(\n",
    "    type(model._preprocessor) == type(models[0]._preprocessor)\n",
    "    and model.model_info.InputH == models[0].model_info.InputH\n",
    "    and model.model_info.InputW == models[0].model_info.InputW\n",
    "    for model in models[1:]\n",
    ")\n",
    "\n",
    "# create video source gizmos;\n",
    "# stop_composition_on_end=True to stop whole composition when one (shorter) video source ends\n",
    "sources = [\n",
    "    dgstreams.VideoSourceGizmo(src, stop_composition_on_end=True)\n",
    "    for src in video_sources\n",
    "]\n",
    "\n",
    "# create image resizer gizmos, one per video source\n",
    "# (we use separate resizers to do resize only once per source to improve performance)\n",
    "resizers = [dgstreams.AiPreprocessGizmo(models[0]) for _ in video_sources]\n",
    "\n",
    "# create multi-input detector gizmos, one per model\n",
    "detectors = [\n",
    "    dgstreams.AiSimpleGizmo(model, inp_cnt=len(video_sources)) for model in models\n",
    "]\n",
    "\n",
    "# create result combiner gizmo to combine results from all detectors into single result\n",
    "combiner = dgstreams.AiResultCombiningGizmo(len(models))\n",
    "\n",
    "# create multi-window video multiplexing display gizmo\n",
    "win_captions = [f\"Stream #{i}: {str(src)}\" for i, src in enumerate(video_sources)]\n",
    "display = dgstreams.VideoDisplayGizmo(\n",
    "    win_captions, show_ai_overlay=True, show_fps=True, multiplex=True\n",
    ")\n",
    "\n",
    "# connect all gizmos in the pipeline\n",
    "# source[i] -> resizer[i] -> detector[j] -> combiner -> display\n",
    "pipeline = (\n",
    "    # each source is connected to corresponding resizer\n",
    "    (source >> resizer for source, resizer in zip(sources, resizers)),\n",
    "    # each resizer is connected to every detector\n",
    "    (\n",
    "        resizer >> detector[ri]\n",
    "        for detector in detectors\n",
    "        for ri, resizer in enumerate(resizers)\n",
    "    ),\n",
    "    # each detector is connected to result combiner\n",
    "    (detector >> combiner[di] for di, detector in enumerate(detectors)),\n",
    "    # result combiner is connected to display\n",
    "    combiner >> display,\n",
    ")\n",
    "\n",
    "# create and start composition with given pipeline\n",
    "dgstreams.Composition(*pipeline).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
