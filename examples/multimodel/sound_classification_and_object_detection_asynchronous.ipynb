{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f257328f",
   "metadata": {},
   "source": [
    "![Degirum banner](https://raw.githubusercontent.com/DeGirum/PySDKExamples/main/images/degirum_banner.png)\n",
    "## Example script illustrating asynchronous parallel execution of sound classification on audio stream and object detection on video stream\n",
    "This notebook is an example of how to use DeGirum PySDK to perform parallel inferences on two asynchronous data streams with different frame rates. \n",
    "To achieve maximum performance this example uses non-blocking batch prediction mode.\n",
    "\n",
    "This script works with the following inference options:\n",
    "\n",
    "1. Run inference on DeGirum Cloud Platform;\n",
    "2. Run inference on DeGirum AI Server deployed on a localhost or on some computer in your LAN or VPN;\n",
    "3. Run inference on DeGirum ORCA accelerator directly installed on your computer.\n",
    "\n",
    "To try different options, you need to specify the appropriate `hw_location` option. \n",
    "\n",
    "When running this notebook locally, you need to specify your cloud API access token in the [env.ini](../../env.ini) file, located in the same directory as this notebook.\n",
    "\n",
    "When running this notebook in Google Colab, the cloud API access token should be stored in a user secret named `DEGIRUM_CLOUD_TOKEN`.\n",
    "\n",
    "**pyaudio package with portaudio is required to run this sample.**\n",
    "\n",
    "The script may use either a web camera or local camera connected to the machine running this code. Alternatively, you may use the video file. The camera index, URL, \n",
    "or file path needs to be specified either in the code below by assigning `camera_id` or in [env.ini](../../env.ini) file by defining `CAMERA_ID` variable and \n",
    "assigning `camera_id = None`.\n",
    "\n",
    "The script may use local microphone connected to the machine running this code. Alternatively, you may use the WAV file.\n",
    "The mic index or WAV filename needs to be specified either in the code below by assigning `audio_id` or in [env.ini](../../env.ini) file by defining `AUDIO_ID` variable \n",
    "and assigning `audio_id = None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure degirum-tools package is installed\n",
    "!pip show degirum-tools || pip install degirum-tools\n",
    "\n",
    "# to install pyaudio package, uncomment the following lines\n",
    "#!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0\n",
    "#!pip show pyaudio || pip install pyaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9de969",
   "metadata": {},
   "source": [
    "#### Specify camera and audio ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9c22fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_source = None  # camera index or URL; 0 to use default local camera, None to take from env.ini file\n",
    "audio_source = None  # mic index or WAV file name; 0 to use default mic, None to take from env.ini file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9699a5-be1b-42ca-af2b-8233eb98d34f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Specify where do you want to run your inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54172f00-f82f-4122-b560-59e172598afb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import degirum as dg\n",
    "import degirum_tools\n",
    "\n",
    "#\n",
    "# Please UNCOMMENT only ONE of the following lines to specify where to run AI inference\n",
    "#\n",
    "\n",
    "hw_location = dg.CLOUD  # <-- on the Cloud Platform\n",
    "# hw_location = degirum_tools.get_ai_server_hostname() # <-- on AI Server deployed in your LAN\n",
    "# hw_location = dg.LOCAL # <-- on ORCA accelerator installed on this computer\n",
    "\n",
    "model_zoo_url = 'degirum/public'\n",
    "sound_model_name = \"mobilenet_v1_yamnet_sound_cls--96x64_quant_n2x_orca1_1\"\n",
    "detection_model_name=\"mobilenet_v2_ssd_coco--300x300_quant_n2x_orca1_1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f15311e-8aed-466d-a11a-bed02b38be33",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The rest of the cells below should run without any modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dd775c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load YAMNET sound classification model for DeGirum Orca AI accelerator\n",
    "# (change model name to \"...n2x_cpu_1\" to run it on CPU)\n",
    "sound_model =dg.load_model(\n",
    "    model_name=sound_model_name,\n",
    "    inference_host_address=hw_location,\n",
    "    zoo_url=model_zoo_url,\n",
    "    token=degirum_tools.get_token(),\n",
    ")\n",
    "\n",
    "# load MobileNetv2+SSD object detection model for DeGirum Orca AI accelerator\n",
    "# (change model name to \"...n2x_cpu_1\" to run it on CPU)\n",
    "detection_model = dg.load_model(\n",
    "    model_name=detection_model_name,\n",
    "    inference_host_address=hw_location,\n",
    "    zoo_url=model_zoo_url,\n",
    "    token=degirum_tools.get_token(),\n",
    ")\n",
    "\n",
    "# set non-blocking mode for both models\n",
    "sound_model.non_blocking_batch_predict = True\n",
    "detection_model.non_blocking_batch_predict = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db989e10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_sampling_rate_hz = sound_model.model_info.InputSamplingRate[0]\n",
    "audio_buffer_size = (\n",
    "    sound_model.model_info.InputWaveformSize[0] // 2\n",
    ")  # two read buffers in waveform for half-length overlapping\n",
    "\n",
    "with degirum_tools.Display(\"Async Streams\") as display, degirum_tools.open_audio_stream(\n",
    "    audio_sampling_rate_hz, audio_buffer_size, audio_source\n",
    ") as audio_stream, degirum_tools.open_video_stream(video_source) as video_stream:\n",
    "    # create prediction result generators:\n",
    "    sound_predictor = sound_model.predict_batch(\n",
    "        degirum_tools.audio_overlapped_source(audio_stream, lambda: False, True)\n",
    "    )\n",
    "    detection_predictor = detection_model.predict_batch(\n",
    "        degirum_tools.video_source(video_stream)\n",
    "    )\n",
    "\n",
    "    sound_label = \"\"\n",
    "    try:\n",
    "        while True:  # press 'x' or 'q' to abort\n",
    "            # do asynchronous ML inferences for both models (each one can be None if not ready):\n",
    "            sound_result = next(sound_predictor)\n",
    "            detection_result = next(detection_predictor)\n",
    "\n",
    "            # process sound classification result (just remember the text)\n",
    "            if sound_result is not None:\n",
    "                sound_label = f\"{sound_result.results[0]['label']}: {sound_result.results[0]['score']}\"\n",
    "\n",
    "            # process video detection result (just display the annotated frame)\n",
    "            if detection_result is not None:\n",
    "                img = detection_result.image_overlay\n",
    "                degirum_tools.put_text(\n",
    "                    img,\n",
    "                    sound_label,\n",
    "                    (1, img.shape[0] - 40),\n",
    "                    font_color=(0, 0, 0),\n",
    "                    bg_color=(255, 255, 255),\n",
    "                )\n",
    "                display.show(img)\n",
    "    except StopIteration:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "19d1d53a962d236aa061289c2ac16dc8e6d9648c89fe79f459ae9a3493bc67b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
